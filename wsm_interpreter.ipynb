{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e76cb18-0204-4a83-a2bd-23747cdcedd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet openai langchain langchain_community bertopic openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89549463-9b10-418f-9266-ef9e00056859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ws_df1 = pd.read_csv('/home/jupyter/WSM/data/weak_signal_df_100k1.csv')\n",
    "ws_df2 = pd.read_csv('/home/jupyter/WSM/data/weak_signal_df_100k2.csv')\n",
    "ws_df3 = pd.read_csv('/home/jupyter/WSM/data/weak_signal_df_100k3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103ae700-1dab-4f76-ba13-6e379a445c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baeb614d-bbf5-4635-b108-3e82b23b1779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are receiving the following topic representation within the realm of food safety.\n",
    "Based on this topic representation and your expert knowledge in the food safety domain your task is to infer an emerging food safety risk. Generate a concise statement about the risk.\n",
    "If you can not infer an emerging food safety risk, just say: no issue found.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5bb0542-918e-441e-bd6e-ca5aeee46d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws_df1: ['Emerging food safety risk: The co-manufacturing of soft hamburger and hot dog buns by bakeries for the Hostess brand may pose a risk of Listeria contamination.', 'No issue found.', 'No issue found.']\n",
      "ws_df2: ['No issue found.', 'Emerging food safety risk: There is a potential risk of Listeria contamination in soft hamburger and hotdog buns produced by Hostess or its co-manufacturers and bakeries.', 'No issue found.']\n",
      "ws_df3: ['no issue found', 'No issue found.', 'No issue found.']\n"
     ]
    }
   ],
   "source": [
    "# Set the OpenAI API key\n",
    "client = openai\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Function to generate text based on topic and risk\n",
    "def infer_text(topic):\n",
    "    prompt = prompt_template + f\"\\ntopic: {topic}\\n\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in food safety.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.6,\n",
    "    )\n",
    "    \n",
    "    # Return the generated content\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Function to process any DataFrame\n",
    "def process_dataframe(df):\n",
    "    # Create an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Loop through each row in the DataFrame and generate text\n",
    "    for index, row in df.iterrows():\n",
    "        topic = row['Representation']\n",
    "        \n",
    "        # Generate the text using the infer_text function\n",
    "        text = infer_text(topic)\n",
    "        \n",
    "        # Append the result to the list\n",
    "        results.append(text)\n",
    "    \n",
    "    # Add the generated text to the DataFrame in a new column\n",
    "    df['weak_signal_interpretation'] = results\n",
    "\n",
    "    # Display first n statements\n",
    "    return df[\"weak_signal_interpretation\"].tolist()[:3]\n",
    "\n",
    "# Process each DataFrame\n",
    "print(\"ws_df1:\", process_dataframe(ws_df1))\n",
    "print(\"ws_df2:\", process_dataframe(ws_df2))\n",
    "print(\"ws_df3:\", process_dataframe(ws_df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a93160cf-5833-495f-b7c6-b564d647d141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws_df1.to_excel('/home/jupyter/WSM/data/100k_interpretation1.xlsx', index=False)\n",
    "ws_df2.to_excel('/home/jupyter/WSM/data/100k_interpretation2.xlsx', index=False)\n",
    "ws_df3.to_excel('/home/jupyter/WSM/data/100k_interpretation3.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c95f93e-7efe-4aac-ba3d-a6499c854a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws_df1:   distant_terms most_distant_terms                            core_topic\n",
      "0            []            'soft',  [['hostess',, 'buns',, 'hamburger',]\n",
      "1      ['fpm']]             'fpm']     [['gaza',, 'israel',, 'israeli',]\n",
      "2  ['towards',]         'towards',   [['antwerp',, 'olivieri',, 'crew',]\n",
      "ws_df2:             distant_terms most_distant_terms  \\\n",
      "0            ['reminds',]         'reminds',   \n",
      "1                      []            'soft',   \n",
      "2  ['340',, 'provinces']]             '340',   \n",
      "\n",
      "                                       core_topic  \n",
      "0  [['plaintiff',, 'shareholders',, 'korsinsky',]  \n",
      "1            [['hostess',, 'buns',, 'hamburger',]  \n",
      "2            [['zucchini',, 'foodle',, 'veggie',]  \n",
      "ws_df3:                                        distant_terms most_distant_terms  \\\n",
      "0  [whos',, 'reacting, elements',, 'repeated, par...              'whos   \n",
      "1                                                 []               None   \n",
      "2                                                 []             'pdc',   \n",
      "\n",
      "                           core_topic  \n",
      "0   [['20240521',, '2024, 20240521',]  \n",
      "1  [['brussels, times',, 'brussels',]  \n",
      "2          [['het',, 'juli',, 'van',]  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load the sentence transformer model\n",
    "sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Define a function to get the first three terms from the list\n",
    "def get_first_three_terms(term_list):\n",
    "    # Check if input is a string, if so split it into words\n",
    "    if isinstance(term_list, str):\n",
    "        term_list = term_list.split()  # You can customize the splitting logic if needed\n",
    "    return term_list[:3]\n",
    "\n",
    "def find_distant_terms(terms, n, threshold=0.3):\n",
    "    if isinstance(terms, str):\n",
    "        terms = terms.split()  # Split terms if provided as a single string\n",
    "\n",
    "    if len(terms) < max(3, n):\n",
    "        return []  # Not enough terms to proceed\n",
    "\n",
    "    # Encoding the first three terms and averaging their embeddings\n",
    "    initial_terms_embeddings = sentence_model.encode(terms[:3], convert_to_tensor=True)\n",
    "    \n",
    "    # Ensure the embedding has a batch dimension\n",
    "    if len(initial_terms_embeddings.shape) == 1:\n",
    "        initial_terms_embeddings = initial_terms_embeddings.unsqueeze(0)\n",
    "\n",
    "    cumulated_embedding = torch.mean(initial_terms_embeddings, dim=0)\n",
    "\n",
    "    distant_terms = []\n",
    "    for term in terms[n-1:]:  # Start from the nth term\n",
    "        term_embedding = sentence_model.encode(term, convert_to_tensor=True)\n",
    "\n",
    "        # Ensure term_embedding also has the correct shape\n",
    "        if len(term_embedding.shape) == 1:\n",
    "            term_embedding = term_embedding.unsqueeze(0)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = util.pytorch_cos_sim(cumulated_embedding.unsqueeze(0), term_embedding).item()\n",
    "        if similarity < threshold:\n",
    "            distant_terms.append(term)\n",
    "    return distant_terms\n",
    "\n",
    "def find_most_distant_term(terms, n, threshold=0.4):\n",
    "    if isinstance(terms, str):\n",
    "        terms = terms.split()  # Split terms if provided as a single string\n",
    "\n",
    "    if len(terms) < max(3, n):\n",
    "        return None  # Not enough terms to proceed\n",
    "\n",
    "    # Encoding the first three terms and averaging their embeddings\n",
    "    initial_terms_embeddings = sentence_model.encode(terms[:3], convert_to_tensor=True)\n",
    "\n",
    "    # Ensure the embedding has a batch dimension\n",
    "    if len(initial_terms_embeddings.shape) == 1:\n",
    "        initial_terms_embeddings = initial_terms_embeddings.unsqueeze(0)\n",
    "\n",
    "    cumulated_embedding = torch.mean(initial_terms_embeddings, dim=0)\n",
    "\n",
    "    most_distant_term = None\n",
    "    lowest_similarity = 1.0  # Initialize with the maximum possible similarity\n",
    "\n",
    "    for term in terms[n-1:]:  # Start from the nth term\n",
    "        term_embedding = sentence_model.encode(term, convert_to_tensor=True)\n",
    "\n",
    "        # Ensure term_embedding also has the correct shape\n",
    "        if len(term_embedding.shape) == 1:\n",
    "            term_embedding = term_embedding.unsqueeze(0)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = util.pytorch_cos_sim(cumulated_embedding.unsqueeze(0), term_embedding).item()\n",
    "        if similarity < threshold and similarity < lowest_similarity:\n",
    "            lowest_similarity = similarity\n",
    "            most_distant_term = term\n",
    "\n",
    "    return most_distant_term\n",
    "\n",
    "# Specify the nth term here, e.g., 4th term would be n=4\n",
    "nth_term = 4\n",
    "\n",
    "# Function to process any DataFrame\n",
    "def process_distant_terms(df):\n",
    "    df['distant_terms'] = df['Representation'].apply(lambda x: find_distant_terms(x, nth_term))\n",
    "    df['most_distant_terms'] = df['Representation'].apply(lambda x: find_most_distant_term(x, nth_term))\n",
    "    df['core_topic'] = df['Representation'].apply(get_first_three_terms)\n",
    "    return df[['distant_terms', 'most_distant_terms', 'core_topic']].head(3)  # Return first 3 rows for inspection\n",
    "\n",
    "# Apply the function to ws_df1, ws_df2, and ws_df3\n",
    "print(\"ws_df1:\", process_distant_terms(ws_df1))\n",
    "print(\"ws_df2:\", process_distant_terms(ws_df2))\n",
    "print(\"ws_df3:\", process_distant_terms(ws_df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ffc5ab9-1f0b-4117-94f0-ffe27af8d71e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are receiving two lists of terms. The first list represents a topic in the field of food safety. The second list contains terms that might pose a risk to that topic.\n",
    "Based on your expert knowledge in the food safety domain your task is to infer what kind of food safety risk might occur based on the given data and produce a short statement to illustrate that risk.\n",
    "Example: Topic: ['cruzi', 'chagas', 'trypanosoma'];\n",
    "Risk: ['attalea'];\n",
    "Output: Deforestation effects on Attalea palms and their resident Rhodnius, vectors of Chagas disease, in eastern Amazonia.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "196a0ebd-51ef-4e93-8404-00fecd97f4c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws_df1: ['No issue found.', 'No issue found.', 'No issue found.']\n",
      "ws_df2: ['No issue found.', 'No issue found.', 'No issue found.']\n",
      "ws_df3: ['No issue found.', 'no issue found.', 'no issue found.']\n"
     ]
    }
   ],
   "source": [
    "# Set the OpenAI API key\n",
    "client = openai\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Function to generate text based on topic and risk\n",
    "def infer_text_distant(topic, risk):\n",
    "    prompt = prompt_template + f\"\\ntopic: {topic}\\n\" + f\"\\nrisk: {risk}\\n\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in food safety.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.6,\n",
    "    )\n",
    "    \n",
    "    # Return the generated content\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Function to process each DataFrame\n",
    "def process_infer_text_distant(df):\n",
    "    results = []\n",
    "\n",
    "    # Loop through each row in the DataFrame and generate text\n",
    "    for index, row in df.iterrows():\n",
    "        topic = row['core_topic']  # Assuming 'core_topic' column exists\n",
    "        risk = row['distant_terms']  # Assuming 'distant_terms' column exists\n",
    "        \n",
    "        # Generate the text using the infer_text function\n",
    "        text = infer_text_distant(topic, risk)\n",
    "        \n",
    "        # Append the result to the list\n",
    "        results.append(text)\n",
    "\n",
    "    # Add the generated text to the DataFrame in a new column\n",
    "    df['odd_term_interpretation'] = results\n",
    "\n",
    "    # Return the first 3 rows for inspection\n",
    "    return df['odd_term_interpretation'].tolist()[:3]\n",
    "\n",
    "# Process each DataFrame (ws_df1, ws_df2, ws_df3)\n",
    "print(\"ws_df1:\", process_infer_text_distant(ws_df1))\n",
    "print(\"ws_df2:\", process_infer_text_distant(ws_df2))\n",
    "print(\"ws_df3:\", process_infer_text_distant(ws_df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84a612ed-2b5c-440b-974f-806ab9fab311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws_df1.to_excel('/home/jupyter/WSM/data/100k_odd_term_interpretation1.xlsx', index=False)\n",
    "ws_df2.to_excel('/home/jupyter/WSM/data/100k_odd_term_interpretation2.xlsx', index=False)\n",
    "ws_df3.to_excel('/home/jupyter/WSM/data/100k_odd_term_interpretation3.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
