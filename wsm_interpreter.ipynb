{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76cb18-0204-4a83-a2bd-23747cdcedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet openai langchain langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95f93e-7efe-4aac-ba3d-a6499c854a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Define a function to get the first three terms from the list\n",
    "def get_first_three_terms(term_list):\n",
    "    return term_list[:3]\n",
    "\n",
    "def find_distant_terms(terms, n, threshold=0.3):\n",
    "    if len(terms) < max(3, n):\n",
    "        return []  # Not enough terms to proceed\n",
    "\n",
    "    # Encoding the first three terms and averaging their embeddings\n",
    "    initial_terms_embeddings = sentence_model.encode(terms[:3], convert_to_tensor=True)\n",
    "    cumulated_embedding = torch.mean(initial_terms_embeddings, dim=0)\n",
    "\n",
    "    distant_terms = []\n",
    "    for term in terms[n-1:]:  # Start from the nth term\n",
    "        term_embedding = sentence_model.encode(term, convert_to_tensor=True)\n",
    "        # Calculate cosine similarity\n",
    "        similarity = util.pytorch_cos_sim(cumulated_embedding, term_embedding).item()\n",
    "        if similarity < threshold:\n",
    "            distant_terms.append(term)\n",
    "    return distant_terms\n",
    "\n",
    "def find_most_distant_term(terms, n, threshold=0.4):\n",
    "    if len(terms) < max(3, n):\n",
    "        return None  # Not enough terms to proceed\n",
    "\n",
    "    # Encoding the first three terms and averaging their embeddings\n",
    "    initial_terms_embeddings = sentence_model.encode(terms[:3], convert_to_tensor=True)\n",
    "    cumulated_embedding = torch.mean(initial_terms_embeddings, dim=0)\n",
    "\n",
    "    most_distant_term = None\n",
    "    lowest_similarity = 1.0  # Initialize with the maximum possible similarity\n",
    "\n",
    "    for term in terms[n-1:]:  # Start from the nth term\n",
    "        term_embedding = sentence_model.encode(term, convert_to_tensor=True)\n",
    "        # Calculate cosine similarity\n",
    "        similarity = util.pytorch_cos_sim(cumulated_embedding, term_embedding).item()\n",
    "        if similarity < threshold and similarity < lowest_similarity:\n",
    "            lowest_similarity = similarity\n",
    "            most_distant_term = term\n",
    "\n",
    "    return most_distant_term\n",
    "\n",
    "# Specify the nth term here, e.g., 4th term would be n=4\n",
    "nth_term = 4\n",
    "weak_signal_df['distant_terms'] = weak_signal_df['Representation'].apply(lambda x: find_distant_terms(x, nth_term))\n",
    "weak_signal_df['most_distant_terms'] = weak_signal_df['Representation'].apply(lambda x: find_most_distant_term(x, nth_term))\n",
    "weak_signal_df['core_topic'] = weak_signal_df['Representation'].apply(get_first_three_terms)\n",
    "\n",
    "# Output the DataFrame\n",
    "print(weak_signal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc5ab9-1f0b-4117-94f0-ffe27af8d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are receiving two lists of terms. The first list represents a topic in the field of food safety. The second list contains terms that might pose a risk to that topic.\n",
    "Based on your expert knowledge in the food safety domain your task is to infer what kind of food safety risk might occur based on the given data and produce a short statement to illustrate that risk.\n",
    "Example: Topic: ['cruzi', 'chagas', 'trypanosoma'];\n",
    "Risk: ['attalea'];\n",
    "Output: Deforestation effects on Attalea palms and their resident Rhodnius, vectors of Chagas disease, in eastern Amazonia.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925769c-2a4e-4476-82e7-1f1c0055a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interprete topic representations\n",
    "\n",
    "\n",
    "from langchain import PromptTemplate, OpenAI\n",
    "\n",
    "prompt_template2 = \"\"\"\n",
    "You are receiving the following topic representation within the realm of food contamination.\n",
    "Based on this topic representation and your expert knowledge in the food safety domain your task is to infer an emerging food safety risk.\n",
    "If you can not infer an emerging food safety risk, just say: no issue found.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93973cbe-4c74-4910-a305-293f8a981dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# Function to generate text based on topic\n",
    "def infer_text2(label):\n",
    "    prompt = prompt_template + f\"\\ntopic: {topic}\\n\" + f\"\\nrisk: {risk}\\n\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in food safety.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        temperature=0.6,\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801b252-a5c8-42e0-b77d-ad72e8dda6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each category and generate project descriptions\n",
    "for index, row in weak_signal_df.iterrows():\n",
    "    topic = row['core_topic']\n",
    "    risk = row['distant_terms']\n",
    "    text = infer_text2(topic)\n",
    "    results.append(text)\n",
    "\n",
    "\n",
    "weak_signal_df['odd_term_interpretaton'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de9891-efde-4dcf-8150-8e3abb28f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_signal_df.to_excel('/home/jupyter/WSM/living_lab/results_final.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72368827-9fdf-4756-af45-b3767f7cec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = random_samples['Representation'].isin(weak_signal_df['Representation'])\n",
    "\n",
    "# If any duplicates are found, duplicates will contain True values\n",
    "has_duplicates = duplicates.any()\n",
    "\n",
    "# Display the result\n",
    "if has_duplicates:\n",
    "    print(\"There are identical rows in the 'Representation' column.\")\n",
    "else:\n",
    "    print(\"No identical rows in the 'Representation' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f019dac7-e961-46db-b15e-d726b3efde95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each category and generate project descriptions\n",
    "for index, row in random_samples.iterrows():\n",
    "    topic = row['Representation']\n",
    "    text = infer_text4(topic)\n",
    "    results.append(text)\n",
    "\n",
    "random_samples['interpretation'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66beaade-7fc6-4671-b75b-ee005520f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples.to_excel('/home/jupyter/WSM/living_lab/random_results.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
