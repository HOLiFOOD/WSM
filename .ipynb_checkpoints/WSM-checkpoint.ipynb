{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q3XIVa5xkAZm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q bertopic pandas nltk scikit-learn matplotlib xlwt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-a0xU37Bihqv",
    "outputId": "ff2f2b68-aac7-49b8-a3a2-0d149fe2ead0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-10</td>\n",
       "      <td>Garlic is valued more for its flavoring and us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-10</td>\n",
       "      <td>The present investigation was undertaken to st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Upland areas of southeastern U.S. tidal creek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-12-10</td>\n",
       "      <td>The specific activity of 90Sr in milk and vege...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-06-11</td>\n",
       "      <td>A survey was conducted to evaluate aflatoxin c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12686</th>\n",
       "      <td>2020-09-29</td>\n",
       "      <td>The gluten-free food market is growing with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12687</th>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>Hurricane Sandy made landfall in New Jersey on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12688</th>\n",
       "      <td>2020-08-28</td>\n",
       "      <td>Prevalent risks in meat value-chains of sub-Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12689</th>\n",
       "      <td>2020-06-03</td>\n",
       "      <td>The presence of metal contaminants in agricult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12690</th>\n",
       "      <td>2019-03-04</td>\n",
       "      <td>The organochlorine pollution by chlordecone, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12373 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                           Abstract\n",
       "0      2015-03-10  Garlic is valued more for its flavoring and us...\n",
       "1      2015-03-10  The present investigation was undertaken to st...\n",
       "2      2015-01-01  Upland areas of southeastern U.S. tidal creek ...\n",
       "3      2014-12-10  The specific activity of 90Sr in milk and vege...\n",
       "4      2014-06-11  A survey was conducted to evaluate aflatoxin c...\n",
       "...           ...                                                ...\n",
       "12686  2020-09-29  The gluten-free food market is growing with th...\n",
       "12687  2020-09-02  Hurricane Sandy made landfall in New Jersey on...\n",
       "12688  2020-08-28  Prevalent risks in meat value-chains of sub-Sa...\n",
       "12689  2020-06-03  The presence of metal contaminants in agricult...\n",
       "12690  2019-03-04  The organochlorine pollution by chlordecone, a...\n",
       "\n",
       "[12373 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Paper = pd.read_csv('/home/jupyter/WSM/data/papers.csv')\n",
    "Paper = Paper.dropna()\n",
    "Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gmean\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to group by a specific time unit (Year, Month, Day)\n",
    "def group_by_time_unit(df, time_unit):\n",
    "    if time_unit == 'year':\n",
    "        return df['Date'].dt.year\n",
    "    elif time_unit == 'month':\n",
    "        return df['Date'].dt.to_period('M')  # Month as YYYY-MM\n",
    "    elif time_unit == 'day':\n",
    "        return df['Date'].dt.date  # Exact day as YYYY-MM-DD\n",
    "    else:\n",
    "        raise ValueError(\"Invalid time_unit. Choose 'year', 'month', or 'day'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_difference(max_time, min_time, time_unit):\n",
    "    if time_unit == 'year':\n",
    "        return max_time - min_time  # Directly subtract the years since they are integers\n",
    "    elif time_unit == 'month':\n",
    "        return (max_time.year - min_time.year) * 12 + (max_time.month - min_time.month)  # Total months\n",
    "    elif time_unit == 'day':\n",
    "        return (max_time - min_time).days  # Difference in days\n",
    "    else:\n",
    "        raise ValueError(\"Invalid time unit for difference. Choose 'year', 'month', or 'day'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the 'Abstract' column to string data type\n",
    "Paper['Abstract'] = Paper['Abstract'].astype(str)\n",
    "\n",
    "# Define a regular expression pattern to match non-Latin characters\n",
    "non_latin_pattern = re.compile(r'[^\\x00-\\x7F]')\n",
    "\n",
    "# Apply the regular expression pattern to the 'Abstract' column\n",
    "Paper = Paper[~Paper['Abstract'].str.contains(non_latin_pattern)]\n",
    "\n",
    "# Set the time unit for grouping (year, month, or day)\n",
    "time_unit = 'year'  # Change this to 'year', 'month', or 'day'\n",
    "\n",
    "# Convert the 'Date' column to datetime and drop invalid dates\n",
    "Paper['Date'] = pd.to_datetime(Paper['Date'], errors='coerce')\n",
    "Paper.dropna(subset=['Date'], inplace=True)\n",
    "\n",
    "# Group data based on the selected time unit\n",
    "Paper['Time'] = group_by_time_unit(Paper, time_unit)\n",
    "\n",
    "# Prepare the abstracts\n",
    "abstracts = Paper[\"Abstract\"].astype('str')\n",
    "text = abstracts.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare embeddings\n",
    "sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = sentence_model.encode(text, show_progress_bar=False)\n",
    "cluster_model = AgglomerativeClustering(distance_threshold=0.2, n_clusters=None)\n",
    "\n",
    "# Train BERTopic\n",
    "topic_model = BERTopic(hdbscan_model=cluster_model).fit(text, embeddings)\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "doc_info = topic_model.get_document_info(text)\n",
    "doc_info[\"ID\"] = doc_info.index\n",
    "doc_info[\"Time\"] = Paper[\"Time\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data based on the selected time unit\n",
    "paper_split = dict(tuple(doc_info.groupby('Time')))\n",
    "\n",
    "# Get the maximum time value (e.g., year, month, or day)\n",
    "max_time = doc_info['Time'].max()\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "dov = pd.DataFrame()\n",
    "\n",
    "# Create a list to store individual DataFrames for each time group (year, month, or day)\n",
    "dfs = []\n",
    "\n",
    "# Set the time weight (tw)\n",
    "tw = 0.05\n",
    "\n",
    "# Iterate over each time group's data\n",
    "for time_value, group in doc_info.groupby('Time'):\n",
    "    topic_count = group['Name'].value_counts().reset_index()\n",
    "    topic_count.columns = ['Name', 'Count']\n",
    "    topic_count['Time'] = time_value\n",
    "\n",
    "    # Get min and max time for this group\n",
    "    min_time = group['Time'].min()\n",
    "    max_time = group['Time'].max()\n",
    "\n",
    "    # Calculate the time difference (based on the time unit)\n",
    "    time_diff = time_difference(max_time, min_time, time_unit)\n",
    "\n",
    "    # Create a DataFrame for the current time's DoV values\n",
    "    df = pd.DataFrame(index=[min_time], columns=topic_count['Name'])\n",
    "    for _, row in topic_count.iterrows():\n",
    "        topic = row['Name']\n",
    "        frequency = row['Count']\n",
    "        # Ensure the time difference is factored in correctly\n",
    "        df[topic] = (frequency / len(group)) * (1 - tw * time_diff)\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the time-based DataFrames into a single DataFrame\n",
    "dov = pd.concat(dfs)\n",
    "\n",
    "# Compute the geometric mean for each topic across time units\n",
    "dov_means = pd.DataFrame(columns=dov.columns)\n",
    "\n",
    "for column in dov.columns:\n",
    "    column_mean = gmean(dov[column].dropna())\n",
    "    dov_means[column] = [column_mean]\n",
    "\n",
    "t_DoV = dov_means.melt(var_name=\"Name\", value_name=\"dov\")\n",
    "\n",
    "# Merge with topic information\n",
    "wsm_dov = pd.merge(t_DoV, topic_info, on='Name')\n",
    "wsm_dov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wsm_dov.to_csv('data/wsm_dov.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create the scatter plot using Plotly\n",
    "fig = px.scatter(wsm_dov, x='Count', y='dov', hover_data=['Name'])\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the desired range for 'dov' and 'frequency'\n",
    "dov_min = 0.0025\n",
    "count_min = 2\n",
    "count_max = 5\n",
    "\n",
    "# Filter the DataFrame based on the range of values\n",
    "weak_signal_df = wsm_dov[(wsm_dov['dov'] >= dov_min) & (wsm_dov['Count'] >= count_min) & (wsm_dov['Count'] <= count_max)]\n",
    "\n",
    "# Get the values from the 'term' column in the filtered DataFrame\n",
    "weak_signals = weak_signal_df['Name'].values.tolist()\n",
    "weak_signal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weak_signal_df.to_csv('data/weak_signal_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m122"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
